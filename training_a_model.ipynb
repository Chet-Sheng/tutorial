{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training a model in TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.learn.python.learn import datasets\n",
    "dataset = datasets.load_iris()\n",
    "\n",
    "# split into input data (x) and GT labels (y)\n",
    "data_x = dataset.data\n",
    "data_y = dataset.target\n",
    "print data_x.shape\n",
    "print data_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into testing and training data. We shuffle the data first, as the dataset is ordered in terms of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_indices = np.random.permutation(data_y.shape[0])\n",
    "\n",
    "train_x = data_x[new_indices[:125],:]\n",
    "train_y = data_y[new_indices[:125]]\n",
    "\n",
    "test_x = data_x[new_indices[125:],:]\n",
    "test_y = data_y[new_indices[125:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Building the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# placeholder for data x (4 attributes), one prediction label\n",
    "x    = tf.placeholder(\"float\", shape=[None, 4])\n",
    "y_GT = tf.placeholder(\"int64\", shape=[None, ])\n",
    "\n",
    "# model parameters\n",
    "n_hidden = 100\n",
    "W_h = tf.Variable(0.1*tf.random_normal([4, n_hidden]), name=\"W_h\")\n",
    "b_h = tf.Variable(tf.random_normal([n_hidden]), name=\"b_h\")\n",
    "hidden_layer = tf.matmul(x, W_h) + b_h\n",
    "\n",
    "# model parameters\n",
    "W = tf.Variable(0.1*tf.random_normal([n_hidden, 3]), name=\"W\")\n",
    "b = tf.Variable(tf.zeros([3]), name=\"b\")\n",
    "\n",
    "# putting the model together\n",
    "y = tf.nn.softmax(tf.matmul(hidden_layer, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Softmax:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Specify loss and training/optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cross-entropy loss\n",
    "#cross_entropy = -tf.reduce_sum(tf.one_hot(y_GT, depth=3) * tf.log(y+1e-10))\n",
    "cross_entropy = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y_GT, depth=3), logits=(tf.matmul(hidden_layer, W) + b)))\n",
    "# define optimizer\n",
    "opt = tf.train.GradientDescentOptimizer(0.001)\n",
    "train_op = opt.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sum:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Launch a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create operation to inialize all variables\n",
    "init_op = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.variables.Variable object at 0x7f9bfebe0ad0>\n"
     ]
    }
   ],
   "source": [
    "# launch session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    print W\n",
    "    #print sess.run(W) #print W.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249.704\n"
     ]
    }
   ],
   "source": [
    "# one step of training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    sess.run(train_op, feed_dict={x: data_x, y_GT: data_y})\n",
    "    \n",
    "    xen = sess.run(cross_entropy, feed_dict={x: data_x, y_GT: data_y})\n",
    "    print xen\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding summary variables that want to track during training\n",
    "\n",
    "We will track the cross-entropy, and accuracy. The log files will be saves in a separate folder **\\tf_logs**.\n",
    "\n",
    "To visualize the plots/learning curves, go to your terminal and execute:\n",
    "\n",
    "```\n",
    ">> tensorboard --logdir=\"\\tf_logs\"\n",
    "```\n",
    "\n",
    "And then open your browser(preferably Chrome) and navigate to: http://127.0.1.1:6006.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the accuracy \n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_GT, tf.argmax(y, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summary \n",
    "tf.scalar_summary(\"cross_entropy\", cross_entropy)\n",
    "tf.scalar_summary(\"accuracy\", accuracy)\n",
    "summary_op = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 -- Cross-entropy: 140.537842\n",
      "Iter 0 -- Accuracy: 0.400000\n",
      "Iter 10 -- Cross-entropy: 12.670593\n",
      "Iter 10 -- Accuracy: 0.960000\n",
      "Iter 20 -- Cross-entropy: 9.923141\n",
      "Iter 20 -- Accuracy: 0.920000\n",
      "Iter 30 -- Cross-entropy: 8.319332\n",
      "Iter 30 -- Accuracy: 0.960000\n",
      "Iter 40 -- Cross-entropy: 7.398432\n",
      "Iter 40 -- Accuracy: 0.920000\n",
      "Iter 50 -- Cross-entropy: 11.380806\n",
      "Iter 50 -- Accuracy: 0.680000\n",
      "Iter 60 -- Cross-entropy: 8.780022\n",
      "Iter 60 -- Accuracy: 0.800000\n",
      "Iter 70 -- Cross-entropy: 9.253835\n",
      "Iter 70 -- Accuracy: 0.800000\n",
      "Iter 80 -- Cross-entropy: 8.677464\n",
      "Iter 80 -- Accuracy: 0.800000\n",
      "Iter 90 -- Cross-entropy: 8.440955\n",
      "Iter 90 -- Accuracy: 0.800000\n",
      "Iter 100 -- Cross-entropy: 8.134848\n",
      "Iter 100 -- Accuracy: 0.800000\n",
      "Iter 110 -- Cross-entropy: 7.754847\n",
      "Iter 110 -- Accuracy: 0.800000\n",
      "Iter 120 -- Cross-entropy: 7.417487\n",
      "Iter 120 -- Accuracy: 0.800000\n",
      "Iter 130 -- Cross-entropy: 7.047817\n",
      "Iter 130 -- Accuracy: 0.800000\n",
      "Iter 140 -- Cross-entropy: 6.641553\n",
      "Iter 140 -- Accuracy: 0.880000\n",
      "Iter 150 -- Cross-entropy: 6.232510\n",
      "Iter 150 -- Accuracy: 0.880000\n",
      "Iter 160 -- Cross-entropy: 5.819290\n",
      "Iter 160 -- Accuracy: 0.880000\n",
      "Iter 170 -- Cross-entropy: 5.380472\n",
      "Iter 170 -- Accuracy: 0.920000\n",
      "Iter 180 -- Cross-entropy: 4.903419\n",
      "Iter 180 -- Accuracy: 0.920000\n",
      "Iter 190 -- Cross-entropy: 4.393711\n",
      "Iter 190 -- Accuracy: 0.920000\n"
     ]
    }
   ],
   "source": [
    "# multiple steps of training\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    summary_writer  = tf.train.SummaryWriter('tf_logs', sess.graph)\n",
    "    for iterIndex in range(200):\n",
    "        sess.run(train_op, feed_dict={x: train_x, y_GT: train_y})\n",
    "        \n",
    "        summary = sess.run(summary_op, feed_dict={x: train_x, y_GT: train_y})\n",
    "        summary_writer.add_summary(summary, iterIndex)\n",
    "        \n",
    "        if iterIndex%10==0:\n",
    "            \n",
    "            xen = sess.run(cross_entropy, feed_dict={x: test_x, y_GT: test_y})\n",
    "            print(\"Iter %d -- Cross-entropy: %f\"%(iterIndex,xen))\n",
    "            \n",
    "            acc = sess.run(accuracy, feed_dict={x: test_x, y_GT: test_y})\n",
    "            print(\"Iter %d -- Accuracy: %f\"%(iterIndex,acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
